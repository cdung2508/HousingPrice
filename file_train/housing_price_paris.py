# -*- coding: utf-8 -*-
"""housing_price_paris.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AnHooZIFTY83IENkGS2H6pb4n0eO0Jtr
"""

# Import data

import pandas as pd

data = pd.read_csv("/content/drive/MyDrive/data/ParisHousing.csv")
data.head()

data['made'].value_counts()

data.info()

data.describe()

# visualize
import matplotlib.pyplot as plt

data.hist(figsize=(20, 15));

plt.figure(figsize=(12, 6))
plt.plot(data['price'], data['squareMeters'])
plt.xlabel('$Price$', fontsize=10)
plt.ylabel('$Square Meters$', rotation=0, fontsize=10)
plt.axis('auto')

# Correlation
corr_matrix = data.corr()
corr_matrix['price'].sort_values(ascending=False)

# Preprocessing
from sklearn.preprocessing import StandardScaler

data['totalRoom'] = data['numberOfRooms'] + data['hasGuestRoom']
data = data.drop(['numberOfRooms', 'hasGuestRoom', 'made', 'attic', 'cityCode'], axis=1)

X = data.drop('price', axis=1)
y = data['price']

std = StandardScaler()
X = std.fit_transform(X)

feature = data.columns.values
feature

# Train test set
import numpy as np

def train_valid_test_split(data, valid_ratio, test_ratio):
    np.random.seed(10)
    total_size = len(data)
    test_size = int(test_ratio * total_size)
    valid_size = int(valid_ratio * total_size)
    train_size = total_size - test_size - valid_size
    shuffle_indices = np.random.permutation(total_size)
    train_set = shuffle_indices[:train_size]
    valid_set = shuffle_indices[train_size:train_size+valid_size]
    test_set = shuffle_indices[-test_size:]
    return data[train_set], data[valid_set], data[test_set]

def train_test_split(data, test_ratio):
    np.random.seed(10)
    total_size = len(data)
    test_size = int(test_ratio * total_size)
    shuffle_indices = np.random.permutation(total_size)
    train_set = shuffle_indices[:-test_size]
    test_set = shuffle_indices[-test_size:]
    return data[train_set], data[test_set]

# X_train, X_valid, X_test = train_valid_test_split(X, 0.2, 0.2)
# y_train, y_valid, y_test = train_valid_test_split(y, 0.2, 0.2)

X_train, X_test = train_test_split(X, 0.2)
y_train, y_test = train_test_split(y, 0.2)

######## Train model using sklearn ###########
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)

lin_reg.intercept_

lin_reg.coef_

# Evaluation using cross-validation
from sklearn.model_selection import cross_val_score
score = cross_val_score(lin_reg, X_train, y_train, scoring='neg_mean_squared_error', cv=10)
lin_rmse_score = np.sqrt(-score)
lin_rmse_score.mean()

# Evaluation on the test set with sklearn
from sklearn.metrics import mean_squared_error
y_predict = lin_reg.predict(X_test)
lin_mse = mean_squared_error(y_test, y_predict)
lin_rmse = np.sqrt(lin_mse)
lin_rmse

###### Gradient descent #######
np.random.seed(10)
eta = 0.01
m = len(X_train)
n_iterations = 1001

X_with_bias = np.c_[np.ones((len(X_train), 1)), X_train]
theta = np.random.randn(X_with_bias.shape[1])

for iteration in range(n_iterations):
    if iteration % 100 == 0:
        loss = np.sqrt(mean_squared_error(y_train, X_with_bias.dot(theta)))
        print(iteration, loss)
    gradients = 2/m * X_with_bias.T.dot(X_with_bias.dot(theta) - y_train)
    theta = theta - eta*gradients

theta

# Evaluations test set with gradient descent
X_test_with_bias = np.c_[np.ones((len(X_test), 1)), X_test]
y_predict = X_test_with_bias.dot(theta)
gd_mse = mean_squared_error(y_test, y_predict)
gd_rmse = np.sqrt(gd_mse)
gd_rmse

def concatenate_with_bias(train, val, test):
    return np.c_[np.ones((len(train), 1)), train], np.c_[np.ones((len(val), 1)), val], np.c_[np.ones((len(test), 1)), test]

###### Early stopping #########

X_train, X_valid, X_test = train_valid_test_split(X, 0.1, 0.2)
y_train, y_valid, y_test = train_valid_test_split(y, 0.1, 0.2)

X_train_with_bias, X_valid_with_bias, X_test_with_bias = concatenate_with_bias(X_train, X_valid, X_test)
np.random.seed(10)
eta = 0.01
n_iterations  = 1001
best_loss = np.infty
m = len(X_train)

theta = np.random.randn(X_train_with_bias.shape[1])

for iteration in range(n_iterations):
    gradients = 2/m * X_train_with_bias.T.dot(X_train_with_bias.dot(theta) - y_train)
    theta = theta - eta * gradients
    
    mse_loss = mean_squared_error(y_valid, X_valid_with_bias.dot(theta))
    loss = np.sqrt(mse_loss)

    if iteration % 10 == 0:
        print(iteration, loss)
    if loss < best_loss:
        best_loss = loss
    else:
        print(iteration - 1, best_loss)
        print(iteration, loss, 'early stopping!')
        break

theta

# Evaluations test set with early stopping
y_predict = X_test_with_bias.dot(theta)
et_mse = mean_squared_error(y_test, y_predict)
et_rmse = np.sqrt(et_mse)
et_rmse

lin_rmse, gd_rmse, et_rmse

import joblib
joblib.dump(lin_reg, 'my_lin_reg_model.pkl')

